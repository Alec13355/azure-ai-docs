---
title: Explore model benchmarks in Azure AI Studio
titleSuffix: Azure AI Studio
description: This article introduces benchmarking capabilities and the model benchmarks experience in Azure AI Studio.
manager: scottpolly
ms.service: azure-ai-studio
ms.custom:
  - ai-learning-hub
ms.topic: concept-article
ms.date: 10/29/2024
ms.reviewer: jcioffi
ms.author: mopeakande
author: msakande
---

# Model benchmarks in Azure AI Studio

[!INCLUDE [feature-preview](../includes/feature-preview.md)]

In Azure AI Studio, you can compare benchmarks across models and datasets available in the industry to decide which one meets your business scenario. You can directly access detailed benchmarking results within the model catalog. Whether you already have models in mind or you're exploring models, the benchmarking data in Azure AI empowers you to make informed decisions quickly and efficiently.

Azure AI supports model benchmarking for select models that are popular and most frequently used. Supported models have a _benchmarks_ icon. From the model catalog, you can also use the filter "Benchmark results" and search functionality to find supported models.

<!-- access-model-catalog-benchmark.png alt-text: Screenshot showing how to access benchmark models in model catalog homepage  .  -->

Model benchmarks help you make informed decisions about the sustainability of models and datasets before you initiate any job. The benchmarks are a curated list of the best-performing models for a task, based on a comprehensive comparison of benchmarking metrics. Azure AI Studio provides the following benchmarks for models based on model catalog collections:

- Benchmarks across large language models (LLMs) and small language models (SLMs)  
- Benchmarks across embeddings models

## Benchmarking of LLMs and SLMs

Model benchmarks assess LLMs and SLMs across the following categories: quality, performance, and cost. The benchmarks are updated regularly as new metrics and datasets are added to existing models, and as new models are added to the model catalog.

### Quality 

The quality of LLMs and SLMs is assessed across various metrics listed below, utilizing two main categories of metrics: Accuracy, and prompt assisted metrics: 

#### Accuracy

| Metric | Description |
|--------|-------------|
| Accuracy | Accuracy scores are available at the dataset and the model levels. At the dataset level, the score is the average value of an accuracy metric computed over all examples in the dataset. The accuracy metric used is exact-match in all cases except for the HumanEval dataset that uses a pass@1 metric. Exact match simply compares model generated text with the correct answer according to the dataset, reporting one if the generated text matches the answer exactly and zero otherwise. Pass@1 measures the proportion of model solutions that pass a set of unit tests in a code generation task. At the model level, the accuracy score is the average of the dataset-level accuracies for each model. |

#### Prompt assisted metrics

| Metric | Description |
|--------|-------------|
| Coherence | Coherence evaluates how well the language model can produce output that flows smoothly, reads naturally, and resembles human-like language. |
| Fluency | Fluency evaluates the language proficiency of a generative AI's predicted answer. It assesses how well the generated text adheres to grammatical rules, syntactic structures, and appropriate usage of vocabulary, resulting in linguistically correct and natural-sounding responses. |
| GPTSimilarity | GPTSimilarity is a measure that quantifies the similarity between a ground truth sentence (or document) and the prediction sentence generated by an AI model. It is calculated by first computing sentence-level embeddings using the embeddings API for both the ground truth and the model's prediction. These embeddings represent high-dimensional vector representations of the sentences, capturing their semantic meaning and context. |
| Groundedness | Groundedness measures how well the language model's generated answers align with information from the input source. |
| Relevance | Relevance measures the extent to which the language model's generated responses are pertinent and directly related to the given questions. |

We also display the quality index as follows:

| Index | Description |
|-------|-------------|
| Quality Index | GPTSimilarity scaled down from 0-1, averaged with our accuracy metrics. The higher, the better. |

When assessing the quality index, we utilize both the measurement of accuracy and GPTSimilarity as our prompt assisted metric. The stability of the GPTSimilarity metric averaging with the accuracy of the model provides an indicator of the overall quality of the model.

### Performance

We utilize two approaches when assessing performance.

#### Approach 1

Our streaming response returns a chunk of 1 or more tokens. We also utilize the following default parameters when we perform our benchmarking:

| Parameter | Value | Applicable For |
|-----------|-------|----------------|
| Region | East US/East US2 | MaaS, Azure OpenAI |
| TPM (Tokens Per Minute) Rate Limit | 30k (180 RPM based on Azure OpenAI) | N/A (Azure MaaS) |
| # of requests | 128 | MaaS, Azure OpenAI |
| Prompt/Context length | Moderate length | MaaS, Azure OpenAI |
| # of tokens processed (Moderate) | 80:20 ratio for Input to Output tokens, i.e., 800 Input tokens to 200 Output tokens. | MaaS, Azure OpenAI |
| # of concurrent requests | 16 | MaaS, Azure OpenAI |
| Data | Synthetic (I/p prompts prepared from static text) | MaaS, Azure OpenAI |
| Deployment Type | Standard | Applicable only for Azure OpenAI (on Oct 24') |
| Streaming | True | Applicable for MaaS, Azure OpenAI. For MaaP models, we must set max_token = 1 to replicate streaming scenario. This would allow to calculate metrics like TTFT for MaaP. |
| Tokenizer | Tiktoken package (Azure OpenAI) | Hugging Face Model Id (Azure MaaS) |

#### Approach 2

Performance metrics are calculated as an aggregate over 14 days, based on 24 trials, with 2 requests per trial. These are sent daily with a one-hour interval between every trial. The following default parameters are used for each request to the model endpoint:

| Parameter | Value | Applicable For |
|-----------|-------|----------------|
| Region | East US/East US2 | MaaS, Azure OpenAI |
| TPM (Tokens Per Minute) Rate Limit | 30k (180 RPM based on Azure OpenAI) | N/A (Azure MaaS) |
| # of requests | 2 requests in a trial for every hour (24 trials per day) | MaaS, Azure OpenAI |
| # of trials/runs | 14 days * 24 trials = 336 | MaaS, Azure OpenAI |
| Prompt/Context length | Moderate length | MaaS, Azure OpenAI |
| # of tokens processed | 80:20 ratio for Input to Output tokens, i.e., 800 Input tokens to 200 Output tokens. | MaaS, Azure OpenAI |
| # of concurrent requests | 1 (Requests are sent sequentially one after another) | MaaS, Azure OpenAI |
| Data | Synthetic (I/p prompts prepared from static text) | MaaS, Azure OpenAI, MaaP |
| Deployment Type | Standard | Applicable only for Azure OpenAI (on Oct 24') |
| Streaming | True | Applicable for MaaS, Azure OpenAI. For MaaP models, we must set max_token = 1 to replicate streaming scenario. This would allow to calculate metrics like TTFT for MaaP. |
| Tokenizer | Tiktoken package (Azure OpenAI) | Hugging Face Model Id (Azure MaaS) |

The performance of LLMs and SLMs is assessed across various metrics listed below:

| Metric | Description |
|--------|-------------|
| Latency mean | Average time in seconds taken for processing a request, computed over multiple requests. To do this, we send a request to the endpoint every hour, for 2 weeks, and compute the average. |
| Latency P50 | 50th percentile value (the median) of latency (the time taken between the request and when we receive the entire response with a successful code). For example: When we send a request to the endpoint, 50% of the requests are completed in 'x' seconds, with 'x' being the latency measurement. |
| Latency P90 | 90th percentile value of latency (the time taken between the request and when we receive the entire response with a successful code). For example: When we send a request to the endpoint, 90% of the requests are completed in 'x' seconds, with 'x' being the latency measurement. |
| Latency P95 | 95th percentile value of latency (the time taken between the request and when we receive the entire response with a successful code). For example: When we send a request to the endpoint, 95% of the requests are complete in 'x' seconds, with 'x' being the latency measurement. |
| Latency P99 | 99th percentile value of latency (the time taken between the request and when we receive the entire response with a successful code). For example: When we send a request to the endpoint, 99% of the requests are complete in 'x' seconds, with 'x' being the latency measurement. |
| Throughput GTPS | GTPS stands for generated tokens per second. This is the number of output tokens that are getting generated per second from the time the request is sent to the endpoint. |
| Throughput TTPS | TTPS stands for total tokens per second. This is the number of total tokens processed per second including both from the input prompt and generated output tokens. |
| Latency TTFT | TTFT stands for total time to first token. This is the time taken for the first token in the response to be returned from the endpoint when streaming is enabled. |
| Time between tokens | This is the time between token received. |

We also display the performance index as follows:

| Index | Description |
|-------|-------------|
| Performance Index | Latency P50. The lower this is, the better. For performance metrics like latency or throughput, the median is often preferred as an index measurement, as it is more robust and less influenced by outliers. This gives a better overall sense of the typical performance and behavior of the model. |

### Cost

Our cost calculations are estimates for utilizing an LLM/SLM model endpoint hosted on Azure AI Platform. We support displaying the cost of MaaS and AOAI models. Please be aware that costs are subject to change. To account for this, we refresh our cost calculations on a cadence of TBD.

The cost of LLMs and SLMs is assessed across various metrics listed below:

| Metric | Description |
|--------|-------------|
| Cost per Input Tokens | Dollar value for pay-as-you-go for 1 million input tokens |
| Cost per Output Tokens | Dollar value for pay-as-you-go for 1 million output tokens |
| Total Price | Dollar value for the sum of cost per input tokens and cost per output tokens, with a ratio of 3:1. |

We also display the cost index as follows:

| Index | Description |
|-------|-------------|
| Cost Index | Total Price. The lower this is, the better. |


## Benchmarking of Embedding Models

Model benchmarks assess embeddings models based on quality.

### Quality

The quality of embedding models is assessed across various metrics listed below:

| Metric | Description |
|--------|-------------|
| Accuracy | Accuracy is the proportion of correct predictions among the total number processed. |
| F1 Score | F1 Score is the weighted mean of the precision and recall, where the best value is 1 (perfect precision and recall), and the worst is 0. |
| Mean Average Precision (MAP) | Mean Average Precision (MAP) evaluates the quality of ranking and recommender systems. It measures both the relevance of suggested items and how good the system is at placing more relevant items at the top. Values can range from 0 to 1, and the higher the MAP, the better the system can place relevant items high in the list. |
| Normalized Discounted Cumulative Gain (NDCG) | Normalized Discounted Cumulative Gain evaluates a machine learning algorithm's ability to sort items based on relevance. It compares rankings to an ideal order where all relevant items are at the top of the list, where k is the list length while evaluating ranking quality. In our benchmarks, k=10, indicated by a metric of ndcg_at_10, meaning that we look at the top 10 items. |
| Precision | Precision measures the model's ability to identify instances of a particular class correctly. Precision shows how often an ML model is correct when predicting the target class. |
| Spearman Correlation | Spearman Correlation based on cosine similarity is calculated by first computing the cosine similarity between variables, then ranking these scores and using the ranks to compute the Spearman Correlation. |
| V Measure | V measure is a metric used to evaluate the quality of clustering. It is calculated as a harmonic mean of homogeneity and completeness, ensuring a balance between the two for a meaningful score. Possible scores lie between 0 and 1, with 1 being perfectly complete labeling. |

### How scores are calculated

#### Individual Scores

Benchmark results originate from public datasets that are commonly used for language model evaluation. In most cases, the data is hosted in GitHub repositories maintained by the creators or curators of the data. Azure AI evaluation pipelines download data from their original sources, extract prompts from each example row, generate model responses, and then compute relevant accuracy metrics.

Prompt construction follows best practices for each dataset, set forth by the paper introducing the dataset and industry standards. In most cases, each prompt contains several examples of complete questions and answers, or "shots," to prime the model for the task. The evaluation pipelines create shots by sampling questions and answers from a portion of the data that is held out from evaluation.

#### Indexes

This section explains how indexes are calculated.

- **Quality index:** Our quality index is calculated as the average score across our accuracy measurement and our GPTSimilarity measurement (brought to a scale from 0-1 using min-max scaling), and is provided on a scale from 0 to 1.
- **Performance index:** Our performance index is calculated as the median latency across all the values that we measure.
- **Cost index:** Our cost index is calculated as the total price as the dollar value for the sum of cost per input tokens and cost per output tokens, with a ratio of 3:1.


## Related content

- [How to benchmark models in Azure AI Studio](../how-to/benchmark-model-in-catalog.md)
- [Model catalog and collections in Azure AI Studio](../how-to/model-catalog-overview.md)